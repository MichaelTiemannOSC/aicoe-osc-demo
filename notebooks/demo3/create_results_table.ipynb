{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "395f44cf",
   "metadata": {},
   "source": [
    "# Demo 3 - Data Ingestion\n",
    "\n",
    "This notebook reads the inference from ceph s3 storage for demo2 and will ingest these inference as a table to trino. These tables will be used for creating visualizations using Apache Superset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4bb47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "from dotenv import load_dotenv\n",
    "import trino\n",
    "import pandas as pd\n",
    "import glob\n",
    "import config\n",
    "from src.data.s3_communication import S3Communication, S3FileType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2fe317",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Injecting Credentials\n",
    "\n",
    "In order to run this notebook, we need credentials to connect with S3 storage to retrieve data and the Trino server to create tables.\n",
    "\n",
    "In an automated environment, the credentials can be specified in a pipeline's environment variables or through Openshift secrets.\n",
    "\n",
    "For running the notebook in a local environment, we will define them as environment variables in a `credentials.env` file at the root of the project repository, and load them using dotenv. An example of what the contents of `credentials.env` could look like is shown below\n",
    "\n",
    "```\n",
    "# s3 credentials\n",
    "S3_ENDPOINT=https://s3.us-east-1.amazonaws.com\n",
    "S3_BUCKET=ocp-odh-os-demo-s3\n",
    "AWS_ACCESS_KEY_ID=xxx\n",
    "AWS_SECRET_ACCESS_KEY=xxx\n",
    "\n",
    "# trino credentials\n",
    "TRINO_USER=xxx\n",
    "TRINO_PASSWD=xxx\n",
    "TRINO_HOST=trino-secure-odh-trino.apps.odh-cl1.apps.os-climate.org\n",
    "TRINO_PORT=443\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84efa52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load credentials\n",
    "dotenv_dir = \"/opt/app-root/src/aicoe-osc-demo\"\n",
    "dotenv_path = pathlib.Path(dotenv_dir) / \"credentials.env\"\n",
    "if os.path.exists(dotenv_path):\n",
    "    load_dotenv(dotenv_path=dotenv_path, override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78368e2",
   "metadata": {},
   "source": [
    "## Read Raw Data from S3\n",
    "\n",
    "First, we will read some sample data from s3. We will format the column data types to ensure they can be understood by Trino, as well as rename the columns so that they are compatible with SQL naming conventions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466e8eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init s3 connector\n",
    "s3c = S3Communication(\n",
    "    s3_endpoint_url=os.getenv(\"S3_ENDPOINT\"),\n",
    "    aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "    aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\"),\n",
    "    s3_bucket=os.getenv(\"S3_BUCKET\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269aa2fb-e410-49b5-a80e-42fba3874604",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.getenv(\"AUTOMATION\"):\n",
    "    if not os.path.exists(config.BASE_INFER_KPI_FOLDER):\n",
    "        pathlib.Path(config.BASE_INFER_KPI_FOLDER).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Download a sample dataset file from s3\n",
    "    s3c.download_files_in_prefix_to_dir(\n",
    "        s3_prefix=config.BASE_INFER_KPI_S3_PREFIX,\n",
    "        destination_dir=config.BASE_INFER_KPI_FOLDER\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f4bdf6-8b20-42a4-9a74-552bfbcec2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = glob.glob(str(config.BASE_INFER_KPI_FOLDER / \"*.csv\"))\n",
    "list_of_files =  []\n",
    "\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename, index_col=None, header=0).convert_dtypes().drop(columns=['Unnamed: 0'],axis=1)\n",
    "    list_of_files.append(df)\n",
    "\n",
    "preds_kpi = pd.concat(list_of_files, axis=0, ignore_index=True)\n",
    "\n",
    "len_preds_kpi = len(preds_kpi)\n",
    "\n",
    "# convert columns to specific data types\n",
    "preds_kpi = preds_kpi.convert_dtypes().drop(['index'], axis=1, errors='ignore')\n",
    "preds_kpi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87ac21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Erik Erlandson <eje@redhat.com>\n",
    "\n",
    "_p2smap = {\"string\": \"varchar\", \"Float64\": \"double\", \"Int64\": \"bigint\"}\n",
    "\n",
    "\n",
    "def pandas_type_to_sql(pt):\n",
    "    st = _p2smap.get(pt)\n",
    "    if st is not None:\n",
    "        return st\n",
    "    raise ValueError(\"unexpected pandas column type '{pt}'\".format(pt=pt))\n",
    "\n",
    "\n",
    "# add ability to specify optional dict for specific fields?\n",
    "# if column name is present, use specified value?\n",
    "def generate_table_schema_pairs(df):\n",
    "    ptypes = [str(e) for e in df.dtypes.to_list()]\n",
    "    stypes = [pandas_type_to_sql(e) for e in ptypes]\n",
    "    pz = list(zip(df.columns.to_list(), stypes))\n",
    "    return \",\\n\".join([\"    {n} {t}\".format(n=e[0], t=e[1]) for e in pz])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5230c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a way to examine the structure of a pandas data frame\n",
    "preds_kpi.info(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4078824e",
   "metadata": {},
   "source": [
    "## Save Processed Data to S3\n",
    "\n",
    "Now that our data is in a form ingestible by Trino, we will upload it back into our s3 bucket. This will be the data source for our Trino table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b3795f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parquet has multiple options for appending or updating data\n",
    "# including adding new files, or appending, sharding directory trees, etc\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename, index_col=None, header=0).convert_dtypes().drop(columns=['Unnamed: 0'],axis=1)\n",
    "    ret = s3c.upload_df_to_s3(\n",
    "        df,\n",
    "        s3_prefix=config.BASE_INFER_KPI_TABLE_S3_PREFIX,\n",
    "        s3_key=f\"{os.path.basename(filename).split('.')[0]}.parquet\",\n",
    "        filetype=S3FileType.PARQUET,\n",
    "        index=False,\n",
    "    )\n",
    "    print(ret['ResponseMetadata']['HTTPStatusCode'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b8af31",
   "metadata": {},
   "source": [
    "## Create a Table on Trino\n",
    "\n",
    "Finally, we will create a table in our Trino database that uses the parquet files we uploaded in the previous section as the data source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950868a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use trino password env-var to hold token values\n",
    "JWT_TOKEN = os.environ['TRINO_PASSWD']\n",
    "conn = trino.dbapi.connect(\n",
    "    host=os.environ['TRINO_HOST'],\n",
    "    port=os.environ['TRINO_PORT'],\n",
    "    user=os.environ['TRINO_USER'],\n",
    "    http_scheme='https',\n",
    "    auth=trino.auth.JWTAuthentication(JWT_TOKEN),\n",
    ")\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4860b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a sql schema that will correspond to the data types\n",
    "# of columns in the pandas DF\n",
    "# to-do: add some mechanisms for overriding types, either here\n",
    "# or on the pandas data-frame itself before we write it out\n",
    "schema = generate_table_schema_pairs(preds_kpi)\n",
    "\n",
    "tabledef = \"\"\"create table if not exists osc_datacommons_dev.urgentem.infer_kpi(\n",
    "{schema}\n",
    ") with (\n",
    "    format = 'parquet',\n",
    "    external_location = 's3a://{s3_bucket}/{kpi_table_s3_prefix}/'\n",
    ")\"\"\".format(\n",
    "    schema=schema,\n",
    "    s3_bucket=os.environ[\"S3_BUCKET\"],\n",
    "    kpi_table_s3_prefix=config.BASE_INFER_KPI_TABLE_S3_PREFIX,\n",
    ")\n",
    "# tables created externally may not show up immediately in cloud-beaver\n",
    "cur.execute(tabledef)\n",
    "cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee568250",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check if infer_kpi table is there\n",
    "cur.execute(\"select * from osc_datacommons_dev.urgentem.infer_kpi LIMIT 5\")\n",
    "cur.fetchall()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d0d0a0",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this notebook, we read inference for KPI sustainability report, 2019 which follows the same format as the output of the KPI Inference model in Demo 2. After reading the report, we automatically infer the data schema from the report, preprocess it and create a table in trino that could be used for visualization in Apache Superset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
